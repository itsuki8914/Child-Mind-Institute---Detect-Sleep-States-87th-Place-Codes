{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d12bf6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:20.356008Z",
     "iopub.status.busy": "2023-11-28T09:11:20.355724Z",
     "iopub.status.idle": "2023-11-28T09:11:35.608416Z",
     "shell.execute_reply": "2023-11-28T09:11:35.607389Z"
    },
    "papermill": {
     "duration": 15.264592,
     "end_time": "2023-11-28T09:11:35.611012",
     "exception": false,
     "start_time": "2023-11-28T09:11:20.346420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import joblib\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm \n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy import signal\n",
    "from scipy.signal import argrelmax\n",
    "\n",
    "from math import pi, sqrt, exp\n",
    "import sklearn,sklearn.model_selection\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "from torch import nn,Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from sklearn.metrics import average_precision_score\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import AutoModel\n",
    "\n",
    "from pyarrow.parquet import ParquetFile\n",
    "import pyarrow as pa \n",
    "import ctypes\n",
    "torch.set_num_interop_threads(4)\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b5e9ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:35.628977Z",
     "iopub.status.busy": "2023-11-28T09:11:35.628468Z",
     "iopub.status.idle": "2023-11-28T09:11:35.633597Z",
     "shell.execute_reply": "2023-11-28T09:11:35.632688Z"
    },
    "papermill": {
     "duration": 0.01618,
     "end_time": "2023-11-28T09:11:35.635710",
     "exception": false,
     "start_time": "2023-11-28T09:11:35.619530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fundamental config\n",
    "WORKERS = os.cpu_count()//2\n",
    "N_FOLDS = 5\n",
    "\n",
    "MAX_LEN = 2880 \n",
    "STRIDE = MAX_LEN // 2\n",
    "SEED = 8620"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "661066d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:35.653240Z",
     "iopub.status.busy": "2023-11-28T09:11:35.652944Z",
     "iopub.status.idle": "2023-11-28T09:11:35.666603Z",
     "shell.execute_reply": "2023-11-28T09:11:35.665818Z"
    },
    "papermill": {
     "duration": 0.02497,
     "end_time": "2023-11-28T09:11:35.668902",
     "exception": false,
     "start_time": "2023-11-28T09:11:35.643932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def torch_fix_seed(seed=42):\n",
    "    # Python random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.use_deterministic_algorithms = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "torch_fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "873f74a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:35.686697Z",
     "iopub.status.busy": "2023-11-28T09:11:35.686411Z",
     "iopub.status.idle": "2023-11-28T09:11:35.690654Z",
     "shell.execute_reply": "2023-11-28T09:11:35.689676Z"
    },
    "papermill": {
     "duration": 0.015512,
     "end_time": "2023-11-28T09:11:35.692815",
     "exception": false,
     "start_time": "2023-11-28T09:11:35.677303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = (384, MAX_LEN)\n",
    "HEIGHT = IMG_SIZE[0]\n",
    "WIDTH = IMG_SIZE[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf86d96e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:35.711246Z",
     "iopub.status.busy": "2023-11-28T09:11:35.710950Z",
     "iopub.status.idle": "2023-11-28T09:11:35.755393Z",
     "shell.execute_reply": "2023-11-28T09:11:35.754516Z"
    },
    "papermill": {
     "duration": 0.055739,
     "end_time": "2023-11-28T09:11:35.757317",
     "exception": false,
     "start_time": "2023-11-28T09:11:35.701578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "__author__ = 'ZFTurbo: https://kaggle.com/zfturbo'\n",
    "\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def prefilter_boxes(boxes, scores, labels, weights, thr):\n",
    "    # Create dict with boxes stored by its label\n",
    "    new_boxes = dict()\n",
    "\n",
    "    for t in range(len(boxes)):\n",
    "\n",
    "        if len(boxes[t]) != len(scores[t]):\n",
    "            print('Error. Length of boxes arrays not equal to length of scores array: {} != {}'.format(len(boxes[t]), len(scores[t])))\n",
    "            exit()\n",
    "\n",
    "        if len(boxes[t]) != len(labels[t]):\n",
    "            print('Error. Length of boxes arrays not equal to length of labels array: {} != {}'.format(len(boxes[t]), len(labels[t])))\n",
    "            exit()\n",
    "\n",
    "        for j in range(len(boxes[t])):\n",
    "            score = scores[t][j]\n",
    "            if score < thr:\n",
    "                continue\n",
    "            label = int(labels[t][j])\n",
    "            box_part = boxes[t][j]\n",
    "            x1 = float(box_part[0])\n",
    "            y1 = float(box_part[1])\n",
    "            x2 = float(box_part[2])\n",
    "            y2 = float(box_part[3])\n",
    "\n",
    "            # Box data checks\n",
    "            if x2 < x1:\n",
    "                warnings.warn('X2 < X1 value in box. Swap them.')\n",
    "                x1, x2 = x2, x1\n",
    "            if y2 < y1:\n",
    "                warnings.warn('Y2 < Y1 value in box. Swap them.')\n",
    "                y1, y2 = y2, y1\n",
    "            if x1 < 0:\n",
    "                warnings.warn('X1 < 0 in box. Set it to 0.')\n",
    "                x1 = 0\n",
    "            if x1 > 1:\n",
    "                warnings.warn('X1 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n",
    "                x1 = 1\n",
    "            if x2 < 0:\n",
    "                warnings.warn('X2 < 0 in box. Set it to 0.')\n",
    "                x2 = 0\n",
    "            if x2 > 1:\n",
    "                warnings.warn('X2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n",
    "                print(x2)\n",
    "                x2 = 1\n",
    "            if y1 < 0:\n",
    "                warnings.warn('Y1 < 0 in box. Set it to 0.')\n",
    "                y1 = 0\n",
    "            if y1 > 1:\n",
    "                warnings.warn('Y1 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n",
    "                y1 = 1\n",
    "            if y2 < 0:\n",
    "                warnings.warn('Y2 < 0 in box. Set it to 0.')\n",
    "                y2 = 0\n",
    "            if y2 > 1:\n",
    "                warnings.warn('Y2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n",
    "                y2 = 1\n",
    "            if (x2 - x1) * (y2 - y1) == 0.0:\n",
    "                warnings.warn(\"Zero area box skipped: {}.\".format(box_part))\n",
    "                continue\n",
    "\n",
    "            # [label, score, weight, model index, x1, y1, x2, y2]\n",
    "            b = [int(label), float(score) * weights[t], weights[t], t, x1, y1, x2, y2]\n",
    "            if label not in new_boxes:\n",
    "                new_boxes[label] = []\n",
    "            new_boxes[label].append(b)\n",
    "\n",
    "    # Sort each list in dict by score and transform it to numpy array\n",
    "    for k in new_boxes:\n",
    "        current_boxes = np.array(new_boxes[k])\n",
    "        new_boxes[k] = current_boxes[current_boxes[:, 1].argsort()[::-1]]\n",
    "\n",
    "    return new_boxes\n",
    "\n",
    "\n",
    "def get_weighted_box(boxes, conf_type='avg'):\n",
    "    \"\"\"\n",
    "    Create weighted box for set of boxes\n",
    "    :param boxes: set of boxes to fuse\n",
    "    :param conf_type: type of confidence one of 'avg' or 'max'\n",
    "    :return: weighted box (label, score, weight, model index, x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "\n",
    "    box = np.zeros(8, dtype=np.float32)\n",
    "    conf = 0\n",
    "    conf_list = []\n",
    "    w = 0\n",
    "    for b in boxes:\n",
    "        box[4:] += (b[1] * b[4:])\n",
    "        conf += b[1]\n",
    "        conf_list.append(b[1])\n",
    "        w += b[2]\n",
    "    box[0] = boxes[0][0]\n",
    "    if conf_type in ('avg', 'box_and_model_avg', 'absent_model_aware_avg'):\n",
    "        box[1] = conf / len(boxes)\n",
    "    elif conf_type == 'max':\n",
    "        box[1] = np.array(conf_list).max()\n",
    "    box[2] = w\n",
    "    box[3] = -1 # model index field is retained for consistency but is not used.\n",
    "    box[4:] /= conf\n",
    "    return box\n",
    "\n",
    "\n",
    "def find_matching_box_fast(boxes_list, new_box, match_iou):\n",
    "    \"\"\"\n",
    "        Reimplementation of find_matching_box with numpy instead of loops. Gives significant speed up for larger arrays\n",
    "        (~100x). This was previously the bottleneck since the function is called for every entry in the array.\n",
    "    \"\"\"\n",
    "    def bb_iou_array(boxes, new_box):\n",
    "        # bb interesection over union\n",
    "        xA = np.maximum(boxes[:, 0], new_box[0])\n",
    "        yA = np.maximum(boxes[:, 1], new_box[1])\n",
    "        xB = np.minimum(boxes[:, 2], new_box[2])\n",
    "        yB = np.minimum(boxes[:, 3], new_box[3])\n",
    "\n",
    "        interArea = np.maximum(xB - xA, 0) * np.maximum(yB - yA, 0)\n",
    "\n",
    "        # compute the area of both the prediction and ground-truth rectangles\n",
    "        boxAArea = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "        boxBArea = (new_box[2] - new_box[0]) * (new_box[3] - new_box[1])\n",
    "\n",
    "        iou = interArea / (boxAArea + boxBArea - interArea)\n",
    "\n",
    "        return iou\n",
    "\n",
    "    if boxes_list.shape[0] == 0:\n",
    "        return -1, match_iou\n",
    "\n",
    "    # boxes = np.array(boxes_list)\n",
    "    boxes = boxes_list\n",
    "\n",
    "    ious = bb_iou_array(boxes[:, 4:], new_box[4:])\n",
    "\n",
    "    ious[boxes[:, 0] != new_box[0]] = -1\n",
    "\n",
    "    best_idx = np.argmax(ious)\n",
    "    best_iou = ious[best_idx]\n",
    "\n",
    "    if best_iou <= match_iou:\n",
    "        best_iou = match_iou\n",
    "        best_idx = -1\n",
    "\n",
    "    return best_idx, best_iou\n",
    "\n",
    "\n",
    "def weighted_boxes_fusion(\n",
    "        boxes_list,\n",
    "        scores_list,\n",
    "        labels_list,\n",
    "        weights=None,\n",
    "        iou_thr=0.55,\n",
    "        skip_box_thr=0.0,\n",
    "        conf_type='avg',\n",
    "        allows_overflow=False\n",
    "):\n",
    "    '''\n",
    "    :param boxes_list: list of boxes predictions from each model, each box is 4 numbers.\n",
    "    It has 3 dimensions (models_number, model_preds, 4)\n",
    "    Order of boxes: x1, y1, x2, y2. We expect float normalized coordinates [0; 1]\n",
    "    :param scores_list: list of scores for each model\n",
    "    :param labels_list: list of labels for each model\n",
    "    :param weights: list of weights for each model. Default: None, which means weight == 1 for each model\n",
    "    :param iou_thr: IoU value for boxes to be a match\n",
    "    :param skip_box_thr: exclude boxes with score lower than this variable\n",
    "    :param conf_type: how to calculate confidence in weighted boxes.\n",
    "        'avg': average value,\n",
    "        'max': maximum value,\n",
    "        'box_and_model_avg': box and model wise hybrid weighted average,\n",
    "        'absent_model_aware_avg': weighted average that takes into account the absent model.\n",
    "    :param allows_overflow: false if we want confidence score not exceed 1.0\n",
    "\n",
    "    :return: boxes: boxes coordinates (Order of boxes: x1, y1, x2, y2).\n",
    "    :return: scores: confidence scores\n",
    "    :return: labels: boxes labels\n",
    "    '''\n",
    "\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(boxes_list))\n",
    "    if len(weights) != len(boxes_list):\n",
    "        print('Warning: incorrect number of weights {}. Must be: {}. Set weights equal to 1.'.format(len(weights), len(boxes_list)))\n",
    "        weights = np.ones(len(boxes_list))\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    if conf_type not in ['avg', 'max', 'box_and_model_avg', 'absent_model_aware_avg']:\n",
    "        print('Unknown conf_type: {}. Must be \"avg\", \"max\" or \"box_and_model_avg\", or \"absent_model_aware_avg\"'.format(conf_type))\n",
    "        exit()\n",
    "\n",
    "    filtered_boxes = prefilter_boxes(boxes_list, scores_list, labels_list, weights, skip_box_thr)\n",
    "    if len(filtered_boxes) == 0:\n",
    "        return np.zeros((0, 4)), np.zeros((0,)), np.zeros((0,))\n",
    "\n",
    "    overall_boxes = []\n",
    "    for label in filtered_boxes:\n",
    "        boxes = filtered_boxes[label]\n",
    "        new_boxes = []\n",
    "        weighted_boxes = np.empty((0, 8))\n",
    "\n",
    "        # Clusterize boxes\n",
    "        for j in range(0, len(boxes)):\n",
    "            index, best_iou = find_matching_box_fast(weighted_boxes, boxes[j], iou_thr)\n",
    "\n",
    "            if index != -1:\n",
    "                new_boxes[index].append(boxes[j])\n",
    "                weighted_boxes[index] = get_weighted_box(new_boxes[index], conf_type)\n",
    "            else:\n",
    "                new_boxes.append([boxes[j].copy()])\n",
    "                weighted_boxes = np.vstack((weighted_boxes, boxes[j].copy()))\n",
    "\n",
    "        # Rescale confidence based on number of models and boxes\n",
    "        for i in range(len(new_boxes)):\n",
    "            clustered_boxes = new_boxes[i]\n",
    "            if conf_type == 'box_and_model_avg':\n",
    "                clustered_boxes = np.array(clustered_boxes)\n",
    "                # weighted average for boxes\n",
    "                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weighted_boxes[i, 2]\n",
    "                # identify unique model index by model index column\n",
    "                _, idx = np.unique(clustered_boxes[:, 3], return_index=True)\n",
    "                # rescale by unique model weights\n",
    "                weighted_boxes[i, 1] = weighted_boxes[i, 1] *  clustered_boxes[idx, 2].sum() / weights.sum()\n",
    "            elif conf_type == 'absent_model_aware_avg':\n",
    "                clustered_boxes = np.array(clustered_boxes)\n",
    "                # get unique model index in the cluster\n",
    "                models = np.unique(clustered_boxes[:, 3]).astype(int)\n",
    "                # create a mask to get unused model weights\n",
    "                mask = np.ones(len(weights), dtype=bool)\n",
    "                mask[models] = False\n",
    "                # absent model aware weighted average\n",
    "                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / (weighted_boxes[i, 2] + weights[mask].sum())\n",
    "            elif conf_type == 'max':\n",
    "                weighted_boxes[i, 1] = weighted_boxes[i, 1] / weights.max()\n",
    "            elif not allows_overflow:\n",
    "                weighted_boxes[i, 1] = weighted_boxes[i, 1] * min(len(weights), len(clustered_boxes)) / weights.sum()\n",
    "            else:\n",
    "                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weights.sum()\n",
    "        overall_boxes.append(weighted_boxes)\n",
    "    overall_boxes = np.concatenate(overall_boxes, axis=0)\n",
    "    overall_boxes = overall_boxes[overall_boxes[:, 1].argsort()[::-1]]\n",
    "    boxes = overall_boxes[:, 4:]\n",
    "    scores = overall_boxes[:, 1]\n",
    "    labels = overall_boxes[:, 0]\n",
    "    return boxes, scores, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e34e91d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:35.775178Z",
     "iopub.status.busy": "2023-11-28T09:11:35.774842Z",
     "iopub.status.idle": "2023-11-28T09:11:35.797315Z",
     "shell.execute_reply": "2023-11-28T09:11:35.796485Z"
    },
    "papermill": {
     "duration": 0.033968,
     "end_time": "2023-11-28T09:11:35.799281",
     "exception": false,
     "start_time": "2023-11-28T09:11:35.765313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PATHS:\n",
    "    MAIN_DIR = \"/kaggle/input/child-mind-institute-detect-sleep-states/\"\n",
    "    # CSV FILES : \n",
    "    SUBMISSION = MAIN_DIR + \"sample_submission.csv\"\n",
    "    TRAIN_EVENTS = MAIN_DIR + \"train_events.csv\"\n",
    "    # PARQUET FILES:\n",
    "    TRAIN_SERIES = MAIN_DIR + \"train_series.parquet\"\n",
    "    TEST_SERIES = MAIN_DIR + \"test_series.parquet\"\n",
    "class CFG:\n",
    "    DEMO_MODE = True\n",
    "class data_reader:\n",
    "    def __init__(self, demo_mode):\n",
    "        super().__init__()\n",
    "        # MAPPING FOR DATA LOADING :\n",
    "        self.names_mapping = {\n",
    "            \"submission\" : {\"path\" : PATHS.SUBMISSION, \"is_parquet\" : False, \"has_timestamp\" : False}, \n",
    "            \"train_events\" : {\"path\" : PATHS.TRAIN_EVENTS, \"is_parquet\" : False, \"has_timestamp\" : True},\n",
    "            \"train_series\" : {\"path\" : PATHS.TRAIN_SERIES, \"is_parquet\" : True, \"has_timestamp\" : True},\n",
    "            \"test_series\" : {\"path\" : PATHS.TEST_SERIES, \"is_parquet\" : True, \"has_timestamp\" : True}\n",
    "        }\n",
    "        self.valid_names = [\"submission\", \"train_events\", \"train_series\", \"test_series\"]\n",
    "        self.demo_mode = demo_mode\n",
    "    \n",
    "    def verify(self, data_name):\n",
    "        \"function for data name verification\"\n",
    "        if data_name not in self.valid_names:\n",
    "            print(\"PLEASE ENTER A VALID DATASET NAME, VALID NAMES ARE : \", valid_names)\n",
    "        return\n",
    "    \n",
    "    def cleaning(self, data):\n",
    "        \"cleaning function : drop na values\"\n",
    "        before_cleaning = len(data)\n",
    "        print(\"Number of missing timestamps : \", len(data[data[\"timestamp\"].isna()]))\n",
    "        data = data.dropna(subset=[\"timestamp\"])\n",
    "        after_cleaning = len(data)\n",
    "        print(\"Percentage of removed rows : {:.1f}%\".format(100 * (before_cleaning - after_cleaning) / before_cleaning) )\n",
    "#         print(data.isna().any())\n",
    "#         data = data.bfill()\n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def reduce_memory_usage(data):\n",
    "        \"iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\"\n",
    "        start_mem = data.memory_usage().sum() / 1024**2\n",
    "        print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "        for col in data.columns:\n",
    "            col_type = data[col].dtype    \n",
    "            if col_type != object:\n",
    "                c_min = data[col].min()\n",
    "                c_max = data[col].max()\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        data[col] = data[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        data[col] = data[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        data[col] = data[col].astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        data[col] = data[col].astype(np.int64)  \n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        data[col] = data[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        data[col] = data[col].astype(np.float32)\n",
    "                    else:\n",
    "                        data[col] = data[col].astype(np.float64)\n",
    "            else:\n",
    "                data[col] = data[col].astype('category')\n",
    "\n",
    "        end_mem = data.memory_usage().sum() / 1024**2\n",
    "        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "        return data\n",
    "    \n",
    "    def load_data(self, data_name):\n",
    "        \"function for data loading\"\n",
    "        self.verify(data_name)\n",
    "        data_props = self.names_mapping[data_name]\n",
    "        if data_props[\"is_parquet\"]:\n",
    "            if self.demo_mode:\n",
    "                pf = ParquetFile(data_props[\"path\"]) \n",
    "                demo_rows = next(pf.iter_batches(batch_size=20_000)) \n",
    "                data = pa.Table.from_batches([demo_rows]).to_pandas()\n",
    "            else:\n",
    "                data = pd.read_parquet(data_props[\"path\"])\n",
    "        else:\n",
    "            if self.demo_mode:\n",
    "                data = pd.read_csv(data_props[\"path\"], nrows=20_000)\n",
    "            else:\n",
    "                data = pd.read_csv(data_props[\"path\"])\n",
    "                \n",
    "        gc.collect()\n",
    "        if data_props[\"has_timestamp\"]:\n",
    "            print('cleaning')\n",
    "            data = self.cleaning(data)\n",
    "            gc.collect()\n",
    "        #data = self.reduce_memory_usage(data)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dabf67f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:35.817519Z",
     "iopub.status.busy": "2023-11-28T09:11:35.817236Z",
     "iopub.status.idle": "2023-11-28T09:11:36.444123Z",
     "shell.execute_reply": "2023-11-28T09:11:36.443092Z"
    },
    "papermill": {
     "duration": 0.638397,
     "end_time": "2023-11-28T09:11:36.446020",
     "exception": false,
     "start_time": "2023-11-28T09:11:35.807623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning\n",
      "Number of missing timestamps :  0\n",
      "Percentage of removed rows : 0.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = data_reader(demo_mode=False)\n",
    "test_series = reader.load_data(data_name=\"test_series\")\n",
    "ids = test_series.series_id.unique()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c1ea37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:36.464130Z",
     "iopub.status.busy": "2023-11-28T09:11:36.463544Z",
     "iopub.status.idle": "2023-11-28T09:11:36.468059Z",
     "shell.execute_reply": "2023-11-28T09:11:36.467218Z"
    },
    "papermill": {
     "duration": 0.015512,
     "end_time": "2023-11-28T09:11:36.469981",
     "exception": false,
     "start_time": "2023-11-28T09:11:36.454469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import *\n",
    "from torchvision.models.resnet import *\n",
    "from torchvision.models.detection.backbone_utils import _resnet_fpn_extractor, _validate_trainable_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68f1edb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:36.487592Z",
     "iopub.status.busy": "2023-11-28T09:11:36.487332Z",
     "iopub.status.idle": "2023-11-28T09:11:36.491932Z",
     "shell.execute_reply": "2023-11-28T09:11:36.491067Z"
    },
    "papermill": {
     "duration": 0.015512,
     "end_time": "2023-11-28T09:11:36.493900",
     "exception": false,
     "start_time": "2023-11-28T09:11:36.478388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.efficientnet import EfficientNet_V2_S_Weights, efficientnet_v2_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba7ccccf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:36.511561Z",
     "iopub.status.busy": "2023-11-28T09:11:36.511237Z",
     "iopub.status.idle": "2023-11-28T09:11:36.515503Z",
     "shell.execute_reply": "2023-11-28T09:11:36.514499Z"
    },
    "papermill": {
     "duration": 0.015035,
     "end_time": "2023-11-28T09:11:36.517360",
     "exception": false,
     "start_time": "2023-11-28T09:11:36.502325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from torchvision.models.detection.image_list import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c04dc985",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:36.534780Z",
     "iopub.status.busy": "2023-11-28T09:11:36.534470Z",
     "iopub.status.idle": "2023-11-28T09:11:36.552194Z",
     "shell.execute_reply": "2023-11-28T09:11:36.551255Z"
    },
    "papermill": {
     "duration": 0.028642,
     "end_time": "2023-11-28T09:11:36.554022",
     "exception": false,
     "start_time": "2023-11-28T09:11:36.525380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seq_block(in_c, out_c, ksh=4, usf=2, ksw=3):\n",
    "    padw = (ksw-1)//2\n",
    "    padh = (ksh-usf)// 2\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(in_c),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.ConvTranspose2d(in_c, out_c, kernel_size=(ksh,ksw), stride=(usf,1), padding=(padh,padw)),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Conv2d(out_c, out_c, ksw, padding=padw)\n",
    "    )\n",
    "class Feat2Img(nn.Module):\n",
    "    def __init__(self, in_c=2, hr_feat=32, base=16, ksw=3):\n",
    "        super(Feat2Img, self).__init__()\n",
    "        self.hr_emb = nn.Embedding(24, hr_feat)\n",
    "        self.fc1_hr = nn.Linear(hr_feat, hr_feat)\n",
    "        self.fc2_hr = nn.Linear(hr_feat, hr_feat)\n",
    "        \n",
    "        self.inter_conv = nn.Conv1d(in_c+hr_feat, base*128, kernel_size=1, padding=0)\n",
    "        \n",
    "        usfs = [3, 2, 2, 2, 2, 2, 2, 2]\n",
    "        kshs = [3, 4, 4, 4, 4, 4, 4, 4]\n",
    "        hid_in = [base*128, base*64, base*32, base*16, base*8, base*4, base*2, base]\n",
    "        hid_out = [base*64, base*32, base*16, base*8, base*4, base*2, base, 3]\n",
    "        padw = (ksw-1)//2\n",
    "\n",
    "        self.blks = nn.Sequential(\n",
    "            *[seq_block(hid_in[i], hid_out[i], kshs[i], usfs[i]) for i in range(len(hid_in))]\n",
    "            )\n",
    "        self.shortcuts = nn.Sequential(\n",
    "            *[nn.ConvTranspose2d(hid_in[i], hid_out[i], kernel_size=(kshs[i],ksw), stride=(usfs[i], 1), padding=((kshs[i]-usfs[i])//2,padw)) for i in range(len(hid_in))]\n",
    "            )\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "    \n",
    "    def forward(self, x, t=None):\n",
    "        #x, h = x[...,:-1].float(), x[...,-1].long()\n",
    "        x, h = x[...,:-1].half(), x[...,-1].long()\n",
    "        e = self.hr_emb(h)\n",
    "        e = self.relu(e)\n",
    "        e = self.fc1_hr(e)\n",
    "        e = self.relu(e)\n",
    "        e = self.fc2_hr(e)\n",
    "        x = torch.cat([x, e], dim=-1).permute(0,2,1)\n",
    "        x = self.inter_conv(x)\n",
    "        x = x.unsqueeze(2)\n",
    "        \n",
    "        for s, b in zip(self.shortcuts, self.blks):\n",
    "            x = s(x) + b(x)\n",
    "\n",
    "        image_list = ImageList(x, [[HEIGHT, WIDTH]]*x.size(0))\n",
    "        return image_list, t\n",
    "\n",
    "    def postprocess(\n",
    "        self,\n",
    "        result: List[Dict[str, Tensor]],\n",
    "        image_shapes: List[Tuple[int, int]],\n",
    "        original_image_sizes: List[Tuple[int, int]],\n",
    "    ) -> List[Dict[str, Tensor]]:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5453470b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:36.572427Z",
     "iopub.status.busy": "2023-11-28T09:11:36.571932Z",
     "iopub.status.idle": "2023-11-28T09:11:36.579994Z",
     "shell.execute_reply": "2023-11-28T09:11:36.579179Z"
    },
    "papermill": {
     "duration": 0.020012,
     "end_time": "2023-11-28T09:11:36.582250",
     "exception": false,
     "start_time": "2023-11-28T09:11:36.562238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "def get_model_convnext(in_c, mn='convnextv2_base.fcmae_ft_in22k_in1k_384', pretrained=False):\n",
    "\n",
    "    backbone = timm.create_model(mn, pretrained=pretrained)\n",
    "\n",
    "    backbone.out_channels = 1280\n",
    "    \n",
    "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                     aspect_ratios=((0.5, 1.0, 2.0),))    \n",
    "        \n",
    "    # put the pieces together inside a FasterRCNN model\n",
    "    model = FasterRCNN(backbone,\n",
    "                    num_classes=4,\n",
    "                    rpn_anchor_generator=anchor_generator\n",
    "                      )\n",
    "\n",
    "    model.transform = Feat2Img(in_c)\n",
    "\n",
    "\n",
    "    if 'large' in mn:\n",
    "        model.backbone.head = nn.Conv2d(1536, 1280, kernel_size=(1, 1), stride=(1, 1), bias=True)\n",
    "\n",
    "    elif 'base' in mn:\n",
    "        model.backbone.head = nn.Conv2d(1024, 1280, kernel_size=(1, 1), stride=(1, 1), bias=True)\n",
    "\n",
    "    else:\n",
    "        assert 0  \n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1ba94d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:36.600551Z",
     "iopub.status.busy": "2023-11-28T09:11:36.600223Z",
     "iopub.status.idle": "2023-11-28T09:11:36.679463Z",
     "shell.execute_reply": "2023-11-28T09:11:36.678476Z"
    },
    "papermill": {
     "duration": 0.091363,
     "end_time": "2023-11-28T09:11:36.681620",
     "exception": false,
     "start_time": "2023-11-28T09:11:36.590257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 95.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# submission\n",
    "class SleepTestDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        test_series,\n",
    "        test_ids\n",
    "    ):\n",
    "        self.enmo_mean = np.load('/kaggle/input/cmi-dss-objectdet-approach/enmo_mean.npy')\n",
    "        self.enmo_std = np.load('/kaggle/input/cmi-dss-objectdet-approach/enmo_std.npy')\n",
    "\n",
    "        self.Xs = self.conv_dfs(test_series, test_ids)\n",
    "        self.ids = test_ids\n",
    "        \n",
    "        self.feat_list = np.load('/kaggle/input/cmi-dss-objectdet-approach/feature_list.npy')\n",
    "        self.label_list = ['onset', 'wakeup']\n",
    "        \n",
    "        self.hour_feat = ['hour']\n",
    "        \n",
    "            \n",
    "    def conv_dfs(self, series, ids):\n",
    "        res = []\n",
    "        for j, viz_id in tqdm(enumerate(ids), total=len(ids)):\n",
    "            viz_series = series.loc[(series.series_id==viz_id)].copy().reset_index()\n",
    "            viz_series['dt'] = pd.to_datetime(viz_series.timestamp,format = '%Y-%m-%dT%H:%M:%S%z').astype(\"datetime64[ns, UTC-04:00]\")\n",
    "            viz_series['hour'] = viz_series['dt'].dt.hour\n",
    "            new_df = viz_series[['step', 'anglez', 'enmo', 'hour']]\n",
    "            res.append(new_df)\n",
    "\n",
    "        return res\n",
    "    \n",
    "\n",
    "    def norm_feat_eng(self, X, init=False):\n",
    "        X['anglez'] = X['anglez'] / 90.0\n",
    "        X['enmo_ln1p'] = np.log1p(X['enmo'])\n",
    "        X['enmo'] = (X['enmo']-self.enmo_mean) / (self.enmo_std+1e-12)\n",
    "\n",
    "        X['anglez_2'] = X['anglez'] ** 2 \n",
    "        X['enmo_ln1p_2'] = X['enmo_ln1p'] ** 2\n",
    "        X['enmo_ln1p_05'] = X['enmo_ln1p'] ** 0.5\n",
    "        X['enmo_ln1p_4'] = X['enmo_ln1p'] ** 4\n",
    "\n",
    "        if init:\n",
    "            self.feat_list.append('anglez_2')\n",
    "            self.feat_list.append('enmo_ln1p_2')\n",
    "            self.feat_list.append('enmo_ln1p_05')\n",
    "            self.feat_list.append('enmo_ln1p_4')\n",
    "            self.feat_list.append('enmo_ln1p')\n",
    "\n",
    "        f = X['anglez']\n",
    "        g = X['enmo']\n",
    "        h = X['enmo_ln1p']\n",
    "        n_grads = 2\n",
    "        for i in range(n_grads):\n",
    "            f = np.gradient(f)\n",
    "            g = np.gradient(g)\n",
    "            h = np.gradient(h)\n",
    "            X['anglez_grad_' + str(i+1)] = f\n",
    "            X['enmo_grad_' + str(i+1)] = g\n",
    "            X['enmo_ln1p_grad_' + str(i+1)] = h\n",
    "            if init:\n",
    "                self.feat_list.append('anglez_grad_' + str(i+1))\n",
    "                self.feat_list.append('enmo_grad_' + str(i+1))\n",
    "                self.feat_list.append('enmo_ln1p_grad_' + str(i+1))\n",
    "       \n",
    "        for w in [1, 2, 4, 8, 16, 32]:    \n",
    "            X['anglez_shift_pos_' + str(w)] = X['anglez'].shift(w).fillna(0)\n",
    "            X['anglez_shift_neg_' + str(w)] = X['anglez'].shift(-w).fillna(0)\n",
    "            \n",
    "            X['enmo_shift_pos_' + str(w)] = X['enmo'].shift(w).fillna(0)\n",
    "            X['enmo_shift_neg_' + str(w)] = X['enmo'].shift(-w).fillna(0)\n",
    "\n",
    "            X['enmo_ln1p_shift_pos_' + str(w)] = X['enmo_ln1p'].shift(w).fillna(0)\n",
    "            X['enmo_ln1p_shift_neg_' + str(w)] = X['enmo_ln1p'].shift(-w).fillna(0)            \n",
    "            \n",
    "            if init:\n",
    "                self.feat_list.append('anglez_shift_pos_' + str(w))\n",
    "                self.feat_list.append('anglez_shift_neg_' + str(w))\n",
    "                \n",
    "                self.feat_list.append('enmo_shift_pos_' + str(w))\n",
    "                self.feat_list.append('enmo_shift_neg_' + str(w))\n",
    "\n",
    "                self.feat_list.append('enmo_ln1p_shift_pos_' + str(w))\n",
    "                self.feat_list.append('enmo_ln1p_shift_neg_' + str(w))\n",
    "            \n",
    "        for r in [5, 17, 33, 65, 129]:\n",
    "            tmp_anglez = X['anglez'].rolling(r, center=True)\n",
    "            X[f'anglez_mean_{r}'] = tmp_anglez.mean()\n",
    "            X[f'anglez_std_{r}'] = tmp_anglez.std()\n",
    "            \n",
    "            tmp_enmo = X['enmo'].rolling(r, center=True)\n",
    "            X[f'enmo_mean_{r}'] = tmp_enmo.mean()\n",
    "            X[f'enmo_std_{r}'] = tmp_enmo.std()\n",
    "\n",
    "            tmp_enmo_ln1p = X['enmo_ln1p'].rolling(r, center=True)\n",
    "            X[f'enmo_ln1p_mean_{r}'] = tmp_enmo_ln1p.mean()\n",
    "            X[f'enmo_ln1p_std_{r}'] = tmp_enmo_ln1p.std()\n",
    "            \n",
    "            if init:\n",
    "                self.feat_list.append(f'anglez_mean_{r}')\n",
    "                self.feat_list.append(f'anglez_std_{r}')\n",
    "\n",
    "                self.feat_list.append(f'enmo_mean_{r}')\n",
    "                self.feat_list.append(f'enmo_std_{r}')\n",
    "\n",
    "                self.feat_list.append(f'enmo_ln1p_mean_{r}')\n",
    "                self.feat_list.append(f'enmo_ln1p_std_{r}')\n",
    "\n",
    "        X = X.fillna(0)\n",
    "        return X.astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Xs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.Xs[index].copy()\n",
    "        X = self.norm_feat_eng(X, init=False)\n",
    "        x = X[self.feat_list].values.astype(np.float32)     \n",
    "        t = X[self.hour_feat].values.astype(np.int32)\n",
    "        x = np.concatenate([x, t], axis=1)\n",
    "        x = torch.tensor(x)\n",
    "        return x, self.ids[index]\n",
    "\n",
    "test_ds = SleepTestDataset(test_series, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b50054f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:36.701635Z",
     "iopub.status.busy": "2023-11-28T09:11:36.701297Z",
     "iopub.status.idle": "2023-11-28T09:11:36.709892Z",
     "shell.execute_reply": "2023-11-28T09:11:36.709059Z"
    },
    "papermill": {
     "duration": 0.020881,
     "end_time": "2023-11-28T09:11:36.712107",
     "exception": false,
     "start_time": "2023-11-28T09:11:36.691226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Debug\n",
    "# class SleepTestDataset(Dataset):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         test_series,\n",
    "#         test_ids\n",
    "#     ):\n",
    "#         self.enmo_mean = np.load('/kaggle/input/cmi-dss-objectdet-approach/enmo_mean.npy')\n",
    "#         self.enmo_std = np.load('/kaggle/input/cmi-dss-objectdet-approach/enmo_std.npy')\n",
    "\n",
    "#         self.Xs, self.ids = self.read_csvs(test_series)\n",
    "        \n",
    "        \n",
    "#         self.feat_list = np.load('/kaggle/input/cmi-dss-objectdet-approach/feature_list.npy')\n",
    "#         self.label_list = ['onset', 'wakeup']\n",
    "        \n",
    "#         self.hour_feat = ['hour']\n",
    "        \n",
    "            \n",
    "#     def read_csvs(self, folder):\n",
    "#         res = []\n",
    "#         ids = []\n",
    "#         if type(folder) is str:\n",
    "#             files = sorted(glob.glob(f'{folder}/*.csv'))\n",
    "#         else:\n",
    "#             files = folder\n",
    "#         for i, f in tqdm(enumerate(files), total=len(files), leave=False):\n",
    "#             df = pd.read_csv(f)\n",
    "#             res.append(df)\n",
    "#             name = f.split('/')[-1].split('.')[0]\n",
    "#             ids.append(name)\n",
    "#         return res, ids\n",
    "    \n",
    "\n",
    "#     def norm_feat_eng(self, X, init=False):\n",
    "#         X['anglez'] = X['anglez'] / 90.0\n",
    "#         X['enmo_ln1p'] = np.log1p(X['enmo'])\n",
    "#         X['enmo'] = (X['enmo']-self.enmo_mean) / (self.enmo_std+1e-12)\n",
    "\n",
    "#         X['anglez_2'] = X['anglez'] ** 2 \n",
    "#         X['enmo_ln1p_2'] = X['enmo_ln1p'] ** 2\n",
    "#         X['enmo_ln1p_05'] = X['enmo_ln1p'] ** 0.5\n",
    "#         X['enmo_ln1p_4'] = X['enmo_ln1p'] ** 4\n",
    "\n",
    "#         if init:\n",
    "#             self.feat_list.append('anglez_2')\n",
    "#             self.feat_list.append('enmo_ln1p_2')\n",
    "#             self.feat_list.append('enmo_ln1p_05')\n",
    "#             self.feat_list.append('enmo_ln1p_4')\n",
    "            \n",
    "#             self.feat_list.append('enmo_ln1p')\n",
    "\n",
    "#         f = X['anglez']\n",
    "#         g = X['enmo']\n",
    "#         h = X['enmo_ln1p']\n",
    "#         n_grads = 2\n",
    "#         for i in range(n_grads):\n",
    "#             f = np.gradient(f)\n",
    "#             g = np.gradient(g)\n",
    "#             h = np.gradient(h)\n",
    "#             X['anglez_grad_' + str(i+1)] = f\n",
    "#             X['enmo_grad_' + str(i+1)] = g\n",
    "#             X['enmo_ln1p_grad_' + str(i+1)] = h\n",
    "#             if init:\n",
    "#                 self.feat_list.append('anglez_grad_' + str(i+1))\n",
    "#                 self.feat_list.append('enmo_grad_' + str(i+1))\n",
    "#                 self.feat_list.append('enmo_ln1p_grad_' + str(i+1))\n",
    "       \n",
    "#         for w in [1, 2, 4, 8, 16, 32]:    \n",
    "#             X['anglez_shift_pos_' + str(w)] = X['anglez'].shift(w).fillna(0)\n",
    "#             X['anglez_shift_neg_' + str(w)] = X['anglez'].shift(-w).fillna(0)\n",
    "            \n",
    "#             X['enmo_shift_pos_' + str(w)] = X['enmo'].shift(w).fillna(0)\n",
    "#             X['enmo_shift_neg_' + str(w)] = X['enmo'].shift(-w).fillna(0)\n",
    "\n",
    "#             X['enmo_ln1p_shift_pos_' + str(w)] = X['enmo_ln1p'].shift(w).fillna(0)\n",
    "#             X['enmo_ln1p_shift_neg_' + str(w)] = X['enmo_ln1p'].shift(-w).fillna(0)            \n",
    "            \n",
    "#             if init:\n",
    "#                 self.feat_list.append('anglez_shift_pos_' + str(w))\n",
    "#                 self.feat_list.append('anglez_shift_neg_' + str(w))\n",
    "                \n",
    "#                 self.feat_list.append('enmo_shift_pos_' + str(w))\n",
    "#                 self.feat_list.append('enmo_shift_neg_' + str(w))\n",
    "\n",
    "#                 self.feat_list.append('enmo_ln1p_shift_pos_' + str(w))\n",
    "#                 self.feat_list.append('enmo_ln1p_shift_neg_' + str(w))\n",
    "            \n",
    "#         for r in [5, 17, 33, 65, 129]:\n",
    "#             tmp_anglez = X['anglez'].rolling(r, center=True)\n",
    "#             X[f'anglez_mean_{r}'] = tmp_anglez.mean()\n",
    "#             X[f'anglez_std_{r}'] = tmp_anglez.std()\n",
    "            \n",
    "#             tmp_enmo = X['enmo'].rolling(r, center=True)\n",
    "#             X[f'enmo_mean_{r}'] = tmp_enmo.mean()\n",
    "#             X[f'enmo_std_{r}'] = tmp_enmo.std()\n",
    "\n",
    "#             tmp_enmo_ln1p = X['enmo_ln1p'].rolling(r, center=True)\n",
    "#             X[f'enmo_ln1p_mean_{r}'] = tmp_enmo_ln1p.mean()\n",
    "#             X[f'enmo_ln1p_std_{r}'] = tmp_enmo_ln1p.std()\n",
    "            \n",
    "#             if init:\n",
    "#                 self.feat_list.append(f'anglez_mean_{r}')\n",
    "#                 self.feat_list.append(f'anglez_std_{r}')\n",
    "\n",
    "#                 self.feat_list.append(f'enmo_mean_{r}')\n",
    "#                 self.feat_list.append(f'enmo_std_{r}')\n",
    "\n",
    "#                 self.feat_list.append(f'enmo_ln1p_mean_{r}')\n",
    "#                 self.feat_list.append(f'enmo_ln1p_std_{r}')\n",
    "\n",
    "#         X = X.fillna(0)\n",
    "#         return X.astype(np.float32)\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.Xs)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         X = self.Xs[index].copy()\n",
    "#         X = self.norm_feat_eng(X, init=False)\n",
    "#         x = X[self.feat_list].values.astype(np.float32)     \n",
    "#         t = X[self.hour_feat].values.astype(np.int32)\n",
    "#         x = np.concatenate([x, t], axis=1)\n",
    "#         x = torch.tensor(x)\n",
    "#         return x, self.ids[index]\n",
    "\n",
    "# import glob\n",
    "# csvfiles = sorted(glob.glob('/kaggle/input/detect-sleep-states-dataprepare/train_csvs/*.csv'))[:10]\n",
    "# test_ds = SleepTestDataset(csvfiles, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "628e99b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:36.730636Z",
     "iopub.status.busy": "2023-11-28T09:11:36.730042Z",
     "iopub.status.idle": "2023-11-28T09:11:36.735615Z",
     "shell.execute_reply": "2023-11-28T09:11:36.734678Z"
    },
    "papermill": {
     "duration": 0.016806,
     "end_time": "2023-11-28T09:11:36.737708",
     "exception": false,
     "start_time": "2023-11-28T09:11:36.720902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds.feat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21b7afa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:36.756866Z",
     "iopub.status.busy": "2023-11-28T09:11:36.756599Z",
     "iopub.status.idle": "2023-11-28T09:11:36.867964Z",
     "shell.execute_reply": "2023-11-28T09:11:36.867023Z"
    },
    "papermill": {
     "duration": 0.123349,
     "end_time": "2023-11-28T09:11:36.869990",
     "exception": false,
     "start_time": "2023-11-28T09:11:36.746641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 80])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b214b02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:36.888996Z",
     "iopub.status.busy": "2023-11-28T09:11:36.888435Z",
     "iopub.status.idle": "2023-11-28T09:11:36.893026Z",
     "shell.execute_reply": "2023-11-28T09:11:36.892240Z"
    },
    "papermill": {
     "duration": 0.01616,
     "end_time": "2023-11-28T09:11:36.895084",
     "exception": false,
     "start_time": "2023-11-28T09:11:36.878924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dl = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    pin_memory=True,\n",
    "    num_workers=WORKERS,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8af998b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:36.914262Z",
     "iopub.status.busy": "2023-11-28T09:11:36.913369Z",
     "iopub.status.idle": "2023-11-28T09:11:36.919731Z",
     "shell.execute_reply": "2023-11-28T09:11:36.918835Z"
    },
    "papermill": {
     "duration": 0.017644,
     "end_time": "2023-11-28T09:11:36.921784",
     "exception": false,
     "start_time": "2023-11-28T09:11:36.904140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def before_padding(x, tgt_len=STRIDE):\n",
    "    x = F.pad(x, (0, 0, tgt_len, 0))\n",
    "    return x\n",
    "\n",
    "def after_padding(x, tgt_len=STRIDE):\n",
    "    x = F.pad(x, (0, 0, 0, tgt_len))\n",
    "    return x\n",
    "\n",
    "def padding_(x, tgt_len=MAX_LEN):\n",
    "    res = tgt_len - (x.size(-2) % tgt_len)\n",
    "    x = F.pad(x, (0, 0, 0, res))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6bad76b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:36.940007Z",
     "iopub.status.busy": "2023-11-28T09:11:36.939745Z",
     "iopub.status.idle": "2023-11-28T09:11:36.943562Z",
     "shell.execute_reply": "2023-11-28T09:11:36.942739Z"
    },
    "papermill": {
     "duration": 0.01518,
     "end_time": "2023-11-28T09:11:36.945382",
     "exception": false,
     "start_time": "2023-11-28T09:11:36.930202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc5dc87c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:36.964380Z",
     "iopub.status.busy": "2023-11-28T09:11:36.964111Z",
     "iopub.status.idle": "2023-11-28T09:11:52.843494Z",
     "shell.execute_reply": "2023-11-28T09:11:52.842375Z"
    },
    "papermill": {
     "duration": 15.891206,
     "end_time": "2023-11-28T09:11:52.846093",
     "exception": false,
     "start_time": "2023-11-28T09:11:36.954887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = '/kaggle/input/cmi-dss-objdet-convnextv2b-fcmae/fepoch40.pth'\n",
    "model = get_model_convnext(in_c=len(test_ds.feat_list), mn='convnextv2_base.fcmae_ft_in22k_in1k_384', pretrained=False).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval().half()\n",
    "models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed0db1e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:52.865894Z",
     "iopub.status.busy": "2023-11-28T09:11:52.865604Z",
     "iopub.status.idle": "2023-11-28T09:11:52.870651Z",
     "shell.execute_reply": "2023-11-28T09:11:52.869771Z"
    },
    "papermill": {
     "duration": 0.017035,
     "end_time": "2023-11-28T09:11:52.872705",
     "exception": false,
     "start_time": "2023-11-28T09:11:52.855670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f487a6df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:11:52.891645Z",
     "iopub.status.busy": "2023-11-28T09:11:52.891355Z",
     "iopub.status.idle": "2023-11-28T09:12:07.587829Z",
     "shell.execute_reply": "2023-11-28T09:12:07.586643Z"
    },
    "papermill": {
     "duration": 14.70827,
     "end_time": "2023-11-28T09:12:07.589892",
     "exception": false,
     "start_time": "2023-11-28T09:11:52.881622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>event</th>\n",
       "      <th>step</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [series_id, event, step, score]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:13<00:27, 13.73s/it]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>event</th>\n",
       "      <th>step</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03d92c9f6f8a</td>\n",
       "      <td>onset</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      series_id  event  step   score\n",
       "0  03d92c9f6f8a  onset  68.0  0.0672"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:14<00:05,  5.92s/it]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>event</th>\n",
       "      <th>step</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0402a003dae9</td>\n",
       "      <td>onset</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.076111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      series_id  event   step     score\n",
       "0  0402a003dae9  onset  131.0  0.076111"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:14<00:00,  4.89s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    with tqdm(test_dl, leave=True) as pbar:\n",
    "        for a, (X_batch, idname) in enumerate(pbar):\n",
    "            bl = [[] for _ in range(len(models))]\n",
    "            sl = [[] for _ in range(len(models))]\n",
    "            ll = [[] for _ in range(len(models))]\n",
    "            il = [[] for _ in range(len(models))]\n",
    "            xl = [[] for _ in range(len(models))]\n",
    "            X_batch = X_batch.to(device)\n",
    "            x_seq_len = X_batch.shape[1]\n",
    "\n",
    "            idname = idname[0]\n",
    "\n",
    "            if x_seq_len%MAX_LEN != 0:\n",
    "                X_batch = padding_(X_batch, MAX_LEN)\n",
    "                \n",
    "            new_x_seq_len = X_batch.shape[1]\n",
    "\n",
    "            for indm, m in enumerate(models):\n",
    "                POS = 0\n",
    "                CENTER = MAX_LEN//2\n",
    "                for b in range(0, new_x_seq_len, STRIDE):\n",
    "                    X_chunk = X_batch[:, b : b + MAX_LEN].half()\n",
    "\n",
    "                    p = m(X_chunk)\n",
    "                    \n",
    "                    for c in range(len(p[0]['boxes'])):\n",
    "                        tmp_step = (p[0]['boxes'][c][0] + p[0]['boxes'][c][2]).cpu().numpy().astype(np.float32) // 2\n",
    "                        \n",
    "                        tmp_box = [p[0]['boxes'][c][0].cpu().numpy().astype(np.float32)+POS, \n",
    "                                   p[0]['boxes'][c][1].cpu().numpy().astype(np.float32), \n",
    "                                   p[0]['boxes'][c][2].cpu().numpy().astype(np.float32)+POS, \n",
    "                                   p[0]['boxes'][c][3].cpu().numpy().astype(np.float32)]\n",
    "                        \n",
    "                        tmp_box[0] = tmp_box[0] / X_batch.shape[1]\n",
    "                        tmp_box[1] = tmp_box[1] / HEIGHT\n",
    "                        tmp_box[2] = tmp_box[2] / X_batch.shape[1]\n",
    "                        tmp_box[3] = tmp_box[3] / HEIGHT\n",
    "                        \n",
    "                        tmp_score = p[0]['scores'][c].cpu().numpy().astype(np.float32)\n",
    "                        tmp_label = p[0]['labels'][c].cpu().numpy().astype(np.int32)\n",
    "                        if b==0:\n",
    "                            CONDITION = (0<=(tmp_step+POS)<=CENTER+STRIDE//2)\n",
    "                            \n",
    "                        elif b==new_x_seq_len-STRIDE:\n",
    "                            CONDITION = (0<=(tmp_step+POS)<=x_seq_len)\n",
    "                            \n",
    "                        else:\n",
    "                            CONDITION = (CENTER-STRIDE//2<=(tmp_step+POS)<=CENTER+STRIDE//2)\n",
    "                            \n",
    "                        if CONDITION:\n",
    "                            \n",
    "                            if tmp_label==1 or tmp_label==2:\n",
    "                                ll[indm].append(tmp_label)\n",
    "                                bl[indm].append(tmp_step)\n",
    "                                sl[indm].append(tmp_score)\n",
    "                                il[indm].append(idname)\n",
    "                                xl[indm].append(tmp_box)\n",
    "\n",
    "\n",
    "                    POS += STRIDE\n",
    "                    CENTER += STRIDE\n",
    "                    \n",
    "            boxes, scores, labels = weighted_boxes_fusion(\n",
    "                    boxes_list=xl,\n",
    "                    scores_list=sl,\n",
    "                    labels_list=ll,\n",
    "                    weights=None,\n",
    "                    iou_thr=0.55,\n",
    "                    skip_box_thr=0.005,\n",
    "                    conf_type='avg',\n",
    "                    allows_overflow=False\n",
    "            )\n",
    "            \n",
    "            step_list = []\n",
    "            \n",
    "            for p in range(len(boxes)):\n",
    "                boxes[p][0] *= X_batch.shape[1]\n",
    "                boxes[p][1] *= HEIGHT\n",
    "                boxes[p][2] *= X_batch.shape[1]\n",
    "                boxes[p][3] *= HEIGHT\n",
    "                step_list.append((boxes[p][0]+boxes[p][2])//2)\n",
    "                \n",
    "            label_list = []\n",
    "            \n",
    "            for p in range(len(labels)):\n",
    "                if labels[p]==1:\n",
    "                    label_list.append('onset')\n",
    "                if labels[p]==2:\n",
    "                    label_list.append('wakeup')\n",
    "                    \n",
    "            tmp_sub = pd.DataFrame()\n",
    "            tmp_sub['series_id'] = [idname] * len(boxes)\n",
    "            tmp_sub['event'] = label_list\n",
    "            tmp_sub['step'] = step_list\n",
    "            tmp_sub['score'] = scores\n",
    "            \n",
    "            if len(test_dl)<=10:\n",
    "                display(tmp_sub)\n",
    "            submission = pd.concat([submission, tmp_sub], axis=0)\n",
    "            \n",
    "            del X_batch\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db9ee70b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:12:07.611777Z",
     "iopub.status.busy": "2023-11-28T09:12:07.610919Z",
     "iopub.status.idle": "2023-11-28T09:12:07.625793Z",
     "shell.execute_reply": "2023-11-28T09:12:07.625083Z"
    },
    "papermill": {
     "duration": 0.0274,
     "end_time": "2023-11-28T09:12:07.627672",
     "exception": false,
     "start_time": "2023-11-28T09:12:07.600272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = submission.sort_values(['series_id','step']).reset_index(drop=True)\n",
    "submission['row_id'] = submission.index.astype(int)\n",
    "submission['score'] = submission['score'].fillna(submission['score'].mean())\n",
    "submission = submission[['row_id','series_id','step','event','score']]\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13ab1462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T09:12:07.647737Z",
     "iopub.status.busy": "2023-11-28T09:12:07.647430Z",
     "iopub.status.idle": "2023-11-28T09:12:07.657309Z",
     "shell.execute_reply": "2023-11-28T09:12:07.656460Z"
    },
    "papermill": {
     "duration": 0.022361,
     "end_time": "2023-11-28T09:12:07.659383",
     "exception": false,
     "start_time": "2023-11-28T09:12:07.637022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>series_id</th>\n",
       "      <th>step</th>\n",
       "      <th>event</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>03d92c9f6f8a</td>\n",
       "      <td>68.0</td>\n",
       "      <td>onset</td>\n",
       "      <td>0.067200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0402a003dae9</td>\n",
       "      <td>131.0</td>\n",
       "      <td>onset</td>\n",
       "      <td>0.076111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id     series_id   step  event     score\n",
       "0       0  03d92c9f6f8a   68.0  onset  0.067200\n",
       "1       1  0402a003dae9  131.0  onset  0.076111"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 6589269,
     "sourceId": 53666,
     "sourceType": "competition"
    },
    {
     "datasetId": 4000443,
     "sourceId": 7054767,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4072789,
     "sourceId": 7072009,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4072812,
     "sourceId": 7072047,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 147531007,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30580,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 55.398317,
   "end_time": "2023-11-28T09:12:11.124904",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-28T09:11:15.726587",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
